# -*- coding: utf-8 -*-
"""Agriculture Analysis_Final

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ntlY0duDNPXgXAbivbQ-SA_K783TH-NS
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

df = pd.read_csv('Crop_recommendation.csv')
df.head()

df.shape

df.isnull().sum()

df.describe()

df.info()

# To check for duplicates
df.duplicated().sum()

#Renaming columns
df.columns = ['Nitrogen','Phosphorus','Potassium','Temperature','Humidity','pH','Rainfall','Label']

"""# **Distribution and Density of Values**"""

#distribution and density of values
import warnings
warnings.filterwarnings("ignore")
plt.style.use('dark_background')
sns.set_palette("Set2")
for i in df.columns[:-1]:
    fig,ax=plt.subplots(1,3,figsize=(18,4))
    sns.histplot(data=df,x=i,kde=True,bins=20,ax=ax[0])
    sns.violinplot(data=df,x=i,ax=ax[1])
    sns.boxplot(data=df,x=i,ax=ax[2])
    plt.suptitle(f'Visualizing {i}',size=20)

#alphabetically arranging crops with their mean value
grouped = df.groupby(by='Label').mean().reset_index()
grouped

"""# **Comparision of Mean Attributes of various classes**"""

fig,ax=plt.subplots(7,1,figsize=(25,25))
for index,i in enumerate(grouped.columns[1:]):
    sns.barplot(data=grouped,x='Label',y=i,ax=ax[index])
    plt.suptitle("Comparision of Mean Attributes of various classes",size=25)
    plt.xlabel("")

"""**# Observations:**
*   Cotton requires most Nitrogen.
*   Apple requires most Phosphorus.
*   Grapes require most Potassium.
*   Papaya requires a hot climate.
*   Coconut requires a humid climate.
*   Chickpea requires high pH in soil.
*   Rice requires huge amount of Rainfall.

# **Top 5 most requiring Crops**
"""

# Top 5 most requiring Crops
print(f'--------------------------------')
for i in grouped.columns[1:]:
    print(f'Top 5 Most {i} requiring crops:')
    print(f'--------------------------------')
    for j ,k in grouped.sort_values(by=i,ascending=False)[:5][['Label',i]].values:
        print(f'{j} --> {k}')
    print(f'-------------------------------')

"""# **Top 5 least requiring Crops**"""

# Top 5 least requiring Crops
print(f'--------------------------------')
for i in grouped.columns[1:]:
    print(f'Top 5 Least {i} requiring crops:')
    print(f'--------------------------------')
    for j ,k in grouped.sort_values(by=i)[:5][['Label',i]].values:
        print(f'{j} --> {k}')
    print(f'-------------------------------')

sns.pairplot(data=df,hue='Label')
plt.show()

"""# **Correlation Analysis**"""

# Convert 'Label' column to numerical representation using LabelEncoder
# Correlation Analysis

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['Label'] = le.fit_transform(df['Label'])

plt.figure(figsize=(10,10))
sns.heatmap(df.corr(),annot=True)
plt.show()

"""A strong positive correlation between Potassium and Phosphorus is observed.

# **Count of different Crop classes**
"""

# count of different crop classes
plt.figure(figsize=(15,4))
sns.countplot(data=df,x='Label')
plt.xticks(rotation = 90)
plt.show()

"""The classes are evenly distributed, making accuracy a suitable metric for evaluation.

# **Soil Nutrient Analysis (N, P, K)**
"""

sns.set(style="whitegrid")

# Create subplots to show the distribution of N, P, and K content in the soil
fig, axs = plt.subplots(3, 1, figsize=(10, 12))

# Nitrogen (N) distribution
sns.histplot(df['Nitrogen'], kde=True, color='b', ax=axs[0]) # Changed 'N' to 'Nitrogen'
axs[0].set_title('Distribution of Nitrogen (N) Content in the Soil', fontsize=14)
axs[0].set_xlabel('Nitrogen (N) Content', fontsize=12)

# Phosphorus (P) distribution
sns.histplot(df['Phosphorus'], kde=True, color='g', ax=axs[1]) # Changed 'P' to 'Phosphorus'
axs[1].set_title('Distribution of Phosphorus (P) Content in the Soil', fontsize=14)
axs[1].set_xlabel('Phosphorus (P) Content', fontsize=12)

# Potassium (K) distribution
sns.histplot(df['Potassium'], kde=True, color='r', ax=axs[2]) # Changed 'K' to 'Potassium'
axs[2].set_title('Distribution of Potassium (K) Content in the Soil', fontsize=14)
axs[2].set_xlabel('Potassium (K) Content', fontsize=12)

# Adjust layout
plt.tight_layout()
plt.show()

"""# **Environmental Factor Analysis (Temperature, Humidity, Rainfall)**"""

sns.set(style="whitegrid")

# Create subplots to show the distribution of Temperature, Humidity, and Rainfall
fig, axs = plt.subplots(3, 1, figsize=(10, 12))

# Temperature distribution
sns.histplot(df['Temperature'], kde=True, color='orange', ax=axs[0]) # Changed 'temperature' to 'Temperature'
axs[0].set_title('Distribution of Temperature', fontsize=14)
axs[0].set_xlabel('Temperature (Â°C)', fontsize=12)

# Humidity distribution
sns.histplot(df['Humidity'], kde=True, color='purple', ax=axs[1]) # Changed 'humidity' to 'Humidity'
axs[1].set_title('Distribution of Humidity', fontsize=14)
axs[1].set_xlabel('Humidity (%)', fontsize=12)

# Rainfall distribution
sns.histplot(df['Rainfall'], kde=True, color='blue', ax=axs[2]) # Changed 'rainfall' to 'Rainfall'
axs[2].set_title('Distribution of Rainfall', fontsize=14)
axs[2].set_xlabel('Rainfall (mm)', fontsize=12)

# Adjust layout
plt.tight_layout()
plt.show()

"""# **pH Analysis**"""

sns.set(style="whitegrid")

# Create a plot to show the distribution of pH levels in the soil
plt.figure(figsize=(8, 6))

# pH distribution
sns.histplot(df['pH'], kde=True, color='green') # Changed 'ph' to 'pH'
plt.title('Distribution of Soil pH Levels', fontsize=14)
plt.xlabel('pH', fontsize=12)

# Show plot
plt.tight_layout()
plt.show()

"""# **Crop Suitability Patterns**"""

sns.set(style="whitegrid")

# Create pair plots to show relationships among nutrients and crop types
sns.pairplot(df, hue='Label', vars=['Nitrogen', 'Phosphorus', 'Potassium', 'Temperature', 'Humidity', 'pH', 'Rainfall'])

# Show plot
plt.show()

from sklearn.decomposition import PCA
pca=PCA(n_components=2)
df_pca=pca.fit_transform(df.drop(['Label'],axis=1))
df_pca=pd.DataFrame(df_pca)
fig = px.scatter(x=df_pca[0],y=df_pca[1],color=df['Label'],title="Decomposed using PCA")
fig.show()

pca3=PCA(n_components=3)
df_pca3=pca3.fit_transform(df.drop(['Label'],axis=1))
df_pca3=pd.DataFrame(df_pca3)
fig = px.scatter_3d(x=df_pca3[0],y=df_pca3[1],z=df_pca3[2],color=df['Label'],title=f"Variance Explained : {pca3.explained_variance_ratio_.sum() * 100}%")
fig.show()

fig = px.scatter(x=df['Nitrogen'],y=df['Phosphorus'],color=df['Label'],title="Nitrogen VS Phosphorus")
fig.show()

fig = px.scatter(x=df['Phosphorus'],y=df['Potassium'],color=df['Label'],title="Phosphorus VS Potassium")
fig.show()

fig = px.scatter(x=df['Nitrogen'], y=df['Potassium'], color=df['Label'], title="Nitrogen VS Potassium")
fig.show()

names = df['Label'].unique()

from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()
df['Label']=encoder.fit_transform(df['Label'])
df.head()

X=df.drop(['Label'],axis=1)
y=df['Label']

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# **Logistic Regression**"""

from sklearn.linear_model import LogisticRegression

# Create a logistic regression model
log_model = LogisticRegression(max_iter=1000)
log_model.fit(X_train, y_train)

# Make predictions
y_pred_log = log_model.predict(X_test)

# Calculate metrics
accuracy_log = accuracy_score(y_test, y_pred_log)
precision_log = precision_score(y_test, y_pred_log, average='weighted')
recall_log = recall_score(y_test, y_pred_log, average='weighted')
f1_log = f1_score(y_test, y_pred_log, average='weighted')

print(f'Logistic Regression: Accuracy: {accuracy_log}, Precision: {precision_log}, Recall: {recall_log}, F1-Score: {f1_log}')

"""# **Decision Tree**"""

from sklearn.tree import DecisionTreeClassifier

# Create a decision tree model
dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)

# Make predictions
y_pred_dt = dt_model.predict(X_test)

# Calculate metrics
accuracy_dt = accuracy_score(y_test, y_pred_dt)
precision_dt = precision_score(y_test, y_pred_dt, average='weighted')
recall_dt = recall_score(y_test, y_pred_dt, average='weighted')
f1_dt = f1_score(y_test, y_pred_dt, average='weighted')

print(f'Decision Tree: Accuracy: {accuracy_dt}, Precision: {precision_dt}, Recall: {recall_dt}, F1-Score: {f1_dt}')

"""# **Support Vector Machine**"""

from sklearn.svm import SVC

# Create a support vector classifier
svm_model = SVC(probability=True)
svm_model.fit(X_train, y_train)

# Make predictions
y_pred_svm = svm_model.predict(X_test)

# Calculate metrics
accuracy_svm = accuracy_score(y_test, y_pred_svm)
precision_svm = precision_score(y_test, y_pred_svm, average='weighted')
recall_svm = recall_score(y_test, y_pred_svm, average='weighted')
f1_svm = f1_score(y_test, y_pred_svm, average='weighted')

print(f'Support Vector Machine: Accuracy: {accuracy_svm}, Precision: {precision_svm}, Recall: {recall_svm}, F1-Score: {f1_svm}')

"""# **K-Nearest Neighbors**"""

from sklearn.neighbors import KNeighborsClassifier

# Create a k-NN model
knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)

# Make predictions
y_pred_knn = knn_model.predict(X_test)

# Calculate metrics
accuracy_knn = accuracy_score(y_test, y_pred_knn)
precision_knn = precision_score(y_test, y_pred_knn, average='weighted')
recall_knn = recall_score(y_test, y_pred_knn, average='weighted')
f1_knn = f1_score(y_test, y_pred_knn, average='weighted')

print(f'k-NN: Accuracy: {accuracy_knn}, Precision: {precision_knn}, Recall: {recall_knn}, F1-Score: {f1_knn}')

"""# **Accuracy Comparison of Different Models**"""

# Accuracy scores for each model
accuracy_scores = [accuracy_log, accuracy_dt, accuracy_svm, accuracy_knn]

# Model names
model_names = ['Logistic Regression', 'Decision Tree', 'SVM', 'KNN']

# Create bar chart
plt.figure(figsize=(10, 6))
bars = plt.bar(model_names, accuracy_scores)
plt.xlabel('Model')
plt.ylabel('Accuracy Score')
plt.title('Accuracy Comparison of Different Models')
plt.ylim(0.9, 1.0)  # Set y-axis limits for better visualization

# Add percentages above the bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, round(yval * 100, 2), ha='center', va='bottom')

plt.show()

"""# **Model Performance Comparison**"""

# Model names
model_names = ['Logistic Regression', 'Decision Tree', 'SVM', 'KNN']

# Metrics for each model
accuracy = [accuracy_log, accuracy_dt, accuracy_svm, accuracy_knn]
precision = [precision_log, precision_dt, precision_svm, precision_knn]
recall = [recall_log, recall_dt, recall_svm, recall_knn]
f1 = [f1_log, f1_dt, f1_svm, f1_knn]

# Set the width of the bars
bar_width = 0.2

# Set position of bar on X axis
r1 = np.arange(len(accuracy))
r2 = [x + bar_width for x in r1]
r3 = [x + bar_width for x in r2]
r4 = [x + bar_width for x in r3]

# Make the plot
plt.figure(figsize=(12, 8))
plt.bar(r1, accuracy, color='#7f6d5f', width=bar_width, edgecolor='white', label='Accuracy')
plt.bar(r2, precision, color='#557f2d', width=bar_width, edgecolor='white', label='Precision')
plt.bar(r3, recall, color='#2d7f5e', width=bar_width, edgecolor='white', label='Recall')
plt.bar(r4, f1, color='#5a3c80', width=bar_width, edgecolor='white', label='F1-Score')

# Add xticks on the middle of the group bars
plt.xlabel('Model', fontweight='bold')
plt.xticks([r + bar_width for r in range(len(accuracy))], model_names)

# Set y-axis limits
plt.ylim(0.9, 1.0)

# Create legend & Show graphic
plt.legend()
plt.title('Model Performance Comparison')
plt.show()

"""# **ROC Curves for Different Models**"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize

# Binarize the output
y_test_bin = label_binarize(y_test, classes=np.unique(y_test))
n_classes = y_test_bin.shape[1]

# Dicts to store results for each model
fpr = {}
tpr = {}
roc_auc = {}

# Models
models = {
    'Logistic Regression': log_model,
    'Decision Tree': dt_model,
    'SVM': svm_model,
    'KNN': knn_model
}

# Iterate through each model
for model_name, model in models.items():
    # Get predicted probabilities
    y_prob = model.predict_proba(X_test)

    # Compute ROC curve and ROC area for each class
    fpr[model_name] = dict()
    tpr[model_name] = dict()
    roc_auc[model_name] = dict()
    for i in range(n_classes):
        fpr[model_name][i], tpr[model_name][i], _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
        roc_auc[model_name][i] = auc(fpr[model_name][i], tpr[model_name][i])

    # Compute micro-average ROC curve and ROC area
    fpr[model_name]["micro"], tpr[model_name]["micro"], _ = roc_curve(y_test_bin.ravel(), y_prob.ravel())
    roc_auc[model_name]["micro"] = auc(fpr[model_name]["micro"], tpr[model_name]["micro"])

# Plot ROC curves for each model
plt.figure(figsize=(10, 8))
for model_name in models.keys():
    plt.plot(fpr[model_name]["micro"], tpr[model_name]["micro"],
             label=f'{model_name} (AUC = {roc_auc[model_name]["micro"]:.2f})')

# Plot diagonal line
plt.plot([0, 1], [0, 1], 'k--')

# Set labels and title
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for Different Models')

# Add legend
plt.legend()

# Show plot
plt.show()

"""# **Precision-Recall Curves for Different Models**"""

import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
import numpy as np

# Binarize the output
y_test_bin = label_binarize(y_test, classes=np.unique(y_test))
n_classes = y_test_bin.shape[1]

# Dicts to store results for each model
precision = {}
recall = {}
average_precision = {}

# Models
models = {
    'Logistic Regression': OneVsRestClassifier(log_model),
    'Decision Tree': OneVsRestClassifier(dt_model),
    'SVM': OneVsRestClassifier(svm_model),
    'KNN': OneVsRestClassifier(knn_model)
}

# Iterate through each model
for model_name, model in models.items():
    # Fit the model on the binarized labels
    model.fit(X_train, y_train)

    # Get predicted probabilities
    y_prob = model.predict_proba(X_test)

    # Check for NaN values and handle them (e.g., replace with 0)
    y_prob = np.nan_to_num(y_prob)

    # Compute precision-recall curve and average precision for each class
    precision[model_name] = dict()
    recall[model_name] = dict()
    average_precision[model_name] = dict()
    for i in range(n_classes):
        precision[model_name][i], recall[model_name][i], _ = precision_recall_curve(y_test_bin[:, i], y_prob[:, i])
        average_precision[model_name][i] = average_precision_score(y_test_bin[:, i], y_prob[:, i])

    # Compute micro-average precision-recall curve and average precision
    precision[model_name]["micro"], recall[model_name]["micro"], _ = precision_recall_curve(y_test_bin.ravel(), y_prob.ravel())
    average_precision[model_name]["micro"] = average_precision_score(y_test_bin, y_prob, average="micro")

# Plot precision-recall curves for each model
plt.figure(figsize=(10, 8))
for model_name in models.keys():
    plt.plot(recall[model_name]["micro"], precision[model_name]["micro"],
             label=f'{model_name} (AP = {average_precision[model_name]["micro"]:.2f})')

# Set labels and title
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curves for Different Models')

# Add legend
plt.legend()

# Show plot
plt.show()